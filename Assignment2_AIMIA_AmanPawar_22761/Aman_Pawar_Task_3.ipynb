{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *DS 261: AI for Medical Image Analysis : Assignment 02*\n",
    "*Submitted By: Aman Pawar, Mtech (1st Year), SR NO: 22761, Department of Bioengineering* \n",
    "\n",
    "*Note: Please install the following libraries if not already installed*\n",
    "\n",
    "*Necessary Installs*<br/>\n",
    "```!pip install numpy scipy matplotlib sklearn tqdm torchmetrics``` <br/>\n",
    "```!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118```<br/>\n",
    "\n",
    "*Installs for better visualizations*<br/>\n",
    "```!pip install torchsummary graphviz torchview``` *Note on Windows you must install executable of graphviz*<br/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Task-3: Utilize the predicted masks from Task-2 to classify CT scans into three distinct groups Normal, Mild, and Severe*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on cuda\n"
     ]
    }
   ],
   "source": [
    "# Doing Necessary imports\n",
    "import gc\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.dpi\"]=1200\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "import torchmetrics as tm\n",
    "from torchsummary import summary\n",
    "from torchview import draw_graph\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Working on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    \"\"\"Function to read data\"\"\"\n",
    "    mat = scipy.io.loadmat(path)\n",
    "    image_data = mat[list(mat.keys())[-1]]\n",
    "    print(\"Reading Data...\")\n",
    "    print(f\"Resolution of CT image : {image_data.shape[:2]} \")\n",
    "    print(f\"The number of CT images : {image_data.shape[2]}\\n\")\n",
    "    return image_data\n",
    "\n",
    "def infection_ratio_cluster(data_ct, data_mask):\n",
    "    \"\"\"Function to compute the infection Ratio and cluster in 3 groups\"\"\"\n",
    "    Normal_ct, Mild_ct, Severe_ct = [], [], []\n",
    "    Normal_mask, Mild_mask, Severe_mask = [], [], []\n",
    "\n",
    "    for i in range(data_mask.shape[2]):\n",
    "        slice_data_mask = data_mask[:, :, i]\n",
    "        slice_data_ct = data_ct[:,:,i]\n",
    "        total_pixels = (slice_data_mask > 0).sum()\n",
    "        infected_pixels = ((slice_data_mask > 0) & (slice_data_mask < 2)).sum()\n",
    "        infection_ratio = infected_pixels / total_pixels * 100\n",
    "\n",
    "        # Categorize the slice based on the infection ratio\n",
    "        if infection_ratio == 0:\n",
    "            Normal_mask.append(slice_data_mask)\n",
    "            Normal_ct.append(slice_data_ct)\n",
    "        elif 0 < infection_ratio <= 40:\n",
    "            Mild_mask.append(slice_data_mask)\n",
    "            Mild_ct.append(slice_data_ct)\n",
    "        else:\n",
    "            Severe_mask.append(slice_data_mask)\n",
    "            Severe_ct.append(slice_data_ct)\n",
    "\n",
    "    return np.array(Normal_mask), np.array(Normal_ct), np.array(Mild_mask), np.array(Mild_ct), np.array(Severe_mask), np.array(Severe_ct)\n",
    "\n",
    "class CustomImageDataset_seg(Dataset):\n",
    "    def __init__(self, data, masks, transform=None):\n",
    "        self.data = data\n",
    "        self.masks = masks\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = self.data[index]\n",
    "        masks = self.masks[index]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the the U-Net Architecture\n",
    "class UNet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels=1, out_channels=1, init_features=32):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        features = init_features\n",
    "        self.encoder1 = UNet._block(in_channels, features, name=\"enc1\")\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder2 = UNet._block(features, features * 2, name=\"enc2\")\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder3 = UNet._block(features * 2, features * 4, name=\"enc3\")\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder4 = UNet._block(features * 4, features * 8, name=\"enc4\")\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.bottleneck = UNet._block(features * 8, features * 16, name=\"bottleneck\")\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(\n",
    "            features * 16, features * 8, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder4 = UNet._block((features * 8) * 2, features * 8, name=\"dec4\")\n",
    "        self.upconv3 = nn.ConvTranspose2d(\n",
    "            features * 8, features * 4, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder3 = UNet._block((features * 4) * 2, features * 4, name=\"dec3\")\n",
    "        self.upconv2 = nn.ConvTranspose2d(\n",
    "            features * 4, features * 2, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder2 = UNet._block((features * 2) * 2, features * 2, name=\"dec2\")\n",
    "        self.upconv1 = nn.ConvTranspose2d(\n",
    "            features * 2, features, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder1 = UNet._block(features * 2, features, name=\"dec1\")\n",
    "\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=features, out_channels=out_channels, kernel_size=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.pool1(enc1))\n",
    "        enc3 = self.encoder3(self.pool2(enc2))\n",
    "        enc4 = self.encoder4(self.pool3(enc3))\n",
    "\n",
    "        bottleneck = self.bottleneck(self.pool4(enc4))\n",
    "\n",
    "        dec4 = self.upconv4(bottleneck)\n",
    "        dec4 = torch.cat((dec4, enc4), dim=1)\n",
    "        dec4 = self.decoder4(dec4)\n",
    "        dec3 = self.upconv3(dec4)\n",
    "        dec3 = torch.cat((dec3, enc3), dim=1)\n",
    "        dec3 = self.decoder3(dec3)\n",
    "        dec2 = self.upconv2(dec3)\n",
    "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
    "        dec2 = self.decoder2(dec2)\n",
    "        dec1 = self.upconv1(dec2)\n",
    "        dec1 = torch.cat((dec1, enc1), dim=1)\n",
    "        dec1 = self.decoder1(dec1)\n",
    "        return torch.sigmoid(self.conv(dec1))\n",
    "\n",
    "    @staticmethod\n",
    "    def _block(in_channels, features, name):\n",
    "        return nn.Sequential(\n",
    "            OrderedDict(\n",
    "                [\n",
    "                    (\n",
    "                        name + \"conv1\",\n",
    "                        nn.Conv2d(\n",
    "                            in_channels=in_channels,\n",
    "                            out_channels=features,\n",
    "                            kernel_size=3,\n",
    "                            padding=1,\n",
    "                            bias=False,\n",
    "                        ),\n",
    "                    ),\n",
    "                    (name + \"norm1\", nn.BatchNorm2d(num_features=features)),\n",
    "                    (name + \"relu1\", nn.ReLU(inplace=True)),\n",
    "                    (\n",
    "                        name + \"conv2\",\n",
    "                        nn.Conv2d(\n",
    "                            in_channels=features,\n",
    "                            out_channels=features,\n",
    "                            kernel_size=3,\n",
    "                            padding=1,\n",
    "                            bias=False,\n",
    "                        ),\n",
    "                    ),\n",
    "                    (name + \"norm2\", nn.BatchNorm2d(num_features=features)),\n",
    "                    (name + \"relu2\", nn.ReLU(inplace=True)),\n",
    "                ]\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For parellel training\n",
    "model = UNet(in_channels=1, out_channels=3)\n",
    "model= nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "\n",
    "#Define your loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.tensor([0.85,2.25,1.25])).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Data...\n",
      "Resolution of CT image : (512, 512) \n",
      "The number of CT images : 3554\n",
      "\n",
      "Reading Data...\n",
      "Resolution of CT image : (512, 512) \n",
      "The number of CT images : 3554\n",
      "\n",
      "Number of Normal CT : 1441\n",
      "Number of Mild CT : 1954\n",
      "Number of Severe CT : 159\n",
      "All Images : (3554, 512, 512), Masks : (3554, 512, 512)\n",
      "Total Number of Image in the DataLoader : 3554\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting the path\n",
    "path_ctscan = '/home/user2/AIMIA_AmanPawar/ctscan_hw1.mat'\n",
    "path_mask = '/home/user2/AIMIA_AmanPawar/infmsk_hw1.mat'\n",
    "\n",
    "# Reading the data\n",
    "ctscan_data = read_data(path_ctscan)\n",
    "mask_data = read_data(path_mask)\n",
    "\n",
    "Normal_mask, Normal_ct, Mild_mask, Mild_ct, Severe_mask, Severe_ct = infection_ratio_cluster(ctscan_data, mask_data)\n",
    "print(f\"Number of Normal CT : {len(Normal_mask)}\\nNumber of Mild CT : {len(Mild_mask)}\\nNumber of Severe CT : {len(Severe_mask)}\")\n",
    "\n",
    "del ctscan_data, mask_data\n",
    "gc.collect()\n",
    "\n",
    "# Preparing the dataset for segementation\n",
    "\n",
    "# Concatenate the CT and mask images\n",
    "all_images = np.concatenate((Normal_ct, Mild_ct, Severe_ct), axis=0)\n",
    "all_masks = np.concatenate((Normal_mask, Mild_mask, Severe_mask), axis=0)\n",
    "\n",
    "# Normalizing all Masks\n",
    "all_masks = all_masks/2.\n",
    "print(f\"All Images : {all_images.shape}, Masks : {all_masks.shape}\")\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    transforms.Normalize((0.0,), (1.0,))  # Rescale pixel values to the range [0, 1]\n",
    "])\n",
    "\n",
    "dataset = CustomImageDataset_seg(all_images, all_masks, transform=transform)\n",
    "\n",
    "# Data Loaders\n",
    "batch_size = 256\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "print(f\"Total Number of Image in the DataLoader : {data_loader.dataset.__len__()}\") \n",
    "# Free Memory\n",
    "del all_images, all_masks\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def irc(data_mask):\n",
    "    \"\"\"Function to compute the infection Ratio and cluster in 3 groups\"\"\"\n",
    "   \n",
    "    Normal_mask, Mild_mask, Severe_mask = [], [], []\n",
    "\n",
    "    for i in data_mask:\n",
    "        total_pixels = (i > 0).sum()\n",
    "        infected_pixels = ((i > 0) & (i < 2)).sum()\n",
    "        infection_ratio = infected_pixels / total_pixels * 100\n",
    "\n",
    "        # Categorize the slice based on the infection ratio\n",
    "        if infection_ratio == 0:\n",
    "            Normal_mask.append(i)\n",
    "            \n",
    "        elif 0 < infection_ratio <= 40:\n",
    "            Mild_mask.append(i)\n",
    "    \n",
    "        else:\n",
    "            Severe_mask.append(i)\n",
    "\n",
    "    return np.array(Normal_mask), np.array(Mild_mask), np.array(Severe_mask),\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 14/14 [00:17<00:00,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Normal CT : 1364\n",
      "Number of Mild CT : 2045\n",
      "Number of Severe CT : 145\n",
      "CPU times: user 8min 20s, sys: 16.4 s, total: 8min 36s\n",
      "Wall time: 17.8 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Loading the model:\n",
    "checkpoint_path = \"/home/user2/AIMIA_AmanPawar/model_weights/unet_checkpoint.pth\"\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "best_valid_accuracy = checkpoint['best_valid_accuracy']\n",
    "\n",
    "# Initialize variables\n",
    "num_classes = 3\n",
    "Normal_mask_count, Mild_mask_count, Severe_mask_count = 0, 0, 0\n",
    "data_loader_iter = tqdm(data_loader, desc='Testing', position=0)\n",
    "\n",
    "# Evaluating the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (images, masks) in enumerate(data_loader_iter):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "       \n",
    "        outputs = model(images)\n",
    "        predicted = torch.argmax(outputs*2., 1)\n",
    "\n",
    "        masks = masks.cpu().numpy()\n",
    "        predicted = predicted.cpu().numpy()\n",
    "\n",
    "        Normal_mask, Mild_mask, Severe_mask = irc(predicted)\n",
    "        \n",
    "        Normal_mask_count += len(Normal_mask)\n",
    "        Mild_mask_count += len(Mild_mask)\n",
    "        Severe_mask_count += len(Severe_mask)\n",
    "\n",
    "    print(f\"Number of Normal CT : {Normal_mask_count}\\nNumber of Mild CT : {Mild_mask_count}\\nNumber of Severe CT : {Severe_mask_count}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aman_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
